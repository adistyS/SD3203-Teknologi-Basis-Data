---
jupyter:
  colab:
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
  language_info:
    codemirror_mode:
      name: ipython
      version: 3
    file_extension: .py
    mimetype: text/x-python
    name: python
    nbconvert_exporter: python
    pygments_lexer: ipython3
    version: 3.9.13
  nbformat: 4
  nbformat_minor: 0
---

::: {.cell .markdown id="ytyk9WrlaJ6H"}
## TUGAS TEKNOLOGI BASIS DATA

Nama : Adisty Syawalda Ariyanto \\ NIM : 121450136 \\ Kelas : TBD RB \\
:::

::: {.cell .code execution_count="1" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="z2-QytdCcUEH" outputId="2c231b0c-1f31-4c46-f7cd-87255b3e287e"}
``` python
import requests
import tarfile
import os

# Tentukan URL di mana dataset CIFAR-10 di-host
url = "https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
# Tentukan direktori di mana Anda ingin menyimpan dataset
save_dir = "./cifar-10"

# Buat direktori jika belum ada
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

# Unduh dataset
response = requests.get(url, stream=True)
# Dapatkan nama file dari URL
filename = url.split("/")[-1]

# Simpan dataset ke dalam file
with open(os.path.join(save_dir, filename), 'wb') as f:
    f.write(response.content)

# Ekstrak isi file tar.gz
with tarfile.open(os.path.join(save_dir, filename), 'r:gz') as tar:
    tar.extractall(save_dir)

print("Dataset CIFAR-10 berhasil diunduh dan diekstrak.")
```

::: {.output .stream .stdout}
    Dataset CIFAR-10 berhasil diunduh dan diekstrak.
:::
:::

::: {.cell .code execution_count="2" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="tZpL4Sp-aJ6L" outputId="f53b1414-d336-45e2-b6b3-61cce0f10119"}
``` python
import numpy as np
import pickle
from pathlib import Path

# Path to the unzipped CIFAR data
data_dir = Path("/content/cifar-10/cifar-10-batches-py")

# Unpickle function provided by the CIFAR hosts
def unpickle(file):
    with open(file, "rb") as fo:
        dict = pickle.load(fo, encoding="bytes")
    return dict

images, labels = [], []
for batch in data_dir.glob("data_batch_*"):
    batch_data = unpickle(batch)
    for i, flat_im in enumerate(batch_data[b"data"]):
        im_channels = []
        # Each image is flattened, with channels in order of R, G, B
        for j in range(3):
            im_channels.append(
                flat_im[j * 1024 : (j + 1) * 1024].reshape((32, 32))
            )
        # Reconstruct the original image
        images.append(np.dstack((im_channels)))
        # Save the label
        labels.append(batch_data[b"labels"][i])

print("Loaded CIFAR-10 training set:")
print(f" - np.shape(images)     {np.shape(images)}")
print(f" - np.shape(labels)     {np.shape(labels)}")
```

::: {.output .stream .stdout}
    Loaded CIFAR-10 training set:
     - np.shape(images)     (50000, 32, 32, 3)
     - np.shape(labels)     (50000,)
:::
:::

::: {.cell .markdown id="va-ck2AeaJ6O"}
Pertama, arahkan ke direktori penyimpanan data CIFAR-10 dan definisikan fungsi untuk membaca file pickle. Buat dua list kosong untuk menyimpan gambar dan label. Selanjutnya lakukan iterasi setiap batch dan direktori data. File di unpickle, dan data piksel dibagi tiga bagian dan gabungkan kembali. Terakhir, ukuran 'images' dan 'labels' dicetak untuk memastikan dataset telah dimuat dengan benar.
:::

::: {.cell .code execution_count="3" id="d_T3UbJtaJ6P"}
``` python
from pathlib import Path

disk_dir = Path("data/disk/")
lmdb_dir = Path("data/lmdb/")
hdf5_dir = Path("data/hdf5/")
```
:::

::: {.cell .markdown id="RHfnNorVaJ6P"}
Objek merepresentasikan alamat direktori: `disk_dir` menuju direktori \"data/disk/\", `lmdb_dir` menuju \"data/lmdb/\", dan `hdf5_dir` menuju \"data/hdf5/\". Dengan objek-objek ini, dapat dengan mudah melakukan operasi-operasi terkait file dan direktori seperti manipulasi file, pembuatan direktori baru, atau pemeriksaan keberadaan file.
:::

::: {.cell .code execution_count="4" id="QYfRBklNaJ6P"}
``` python
disk_dir.mkdir(parents=True, exist_ok=True)
lmdb_dir.mkdir(parents=True, exist_ok=True)
hdf5_dir.mkdir(parents=True, exist_ok=True)
```
:::

::: {.cell .markdown id="FKRD6S80aJ6Q"}
Kode menggunakan metode `mkdir()` pada objek `Path` untuk membuat direktori yang ditunjukkan oleh `disk_dir`, `lmdb_dir`, dan `hdf5_dir`. Argumen `parents=True` digunakan untuk membuat direktori induk jika tidak ada, sedangkan `exist_ok=True` memungkinkan pembuatan direktori tersebut meskipun direktori sudah ada sebelumnya tanpa memunculkan pesan error. Jadi, kode ini secara efisien membuat tiga direktori yang diperlukan untuk penyimpanan data dalam proyek.
:::

::: {.cell .code execution_count="5" id="sGl8TLuDaJ6Q"}
``` python
from PIL import Image
import csv

def store_single_disk(image, image_id, label):
    """ Stores a single image as a .png file on disk.
        Parameters:
        ---------------
        image       image array, (32, 32, 3) to be stored
        image_id    integer unique ID for image
        label       image label
    """
    Image.fromarray(image).save(disk_dir / f"{image_id}.png")

    with open(disk_dir / f"{image_id}.csv", "wt") as csvfile:
        writer = csv.writer(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        writer.writerow([label])
```
:::

::: {.cell .markdown id="X_GqrO4baJ6Q"}
Kode merupakan fungsi yang disusun untuk menyimpan gambar dan label ke dalam direktori yang dibuat sebelumnya menggunakan `Path` yang di definisikan. Fungsi `store_single_disk` menerima tiga argumen: gambar sebagai array dengan dimensi (32, 32, 3), ID unik gambar sebagai integer, dan label gambar. Langkah awal, gambar disimpan sebagai file .png di dalam direktori `disk_dir` dengan file yang sesuai dengan ID gambar. Lalu, label disimpan ke dalam file .csv yang diletakkan di dalam direktori yang sama. Fungsi ini, dapat menyimpan gambar-gambar beserta labelnya ke dalam struktur direktori yang telah di siapkan sebelumnya dengan mudah.
:::

::: {.cell .code execution_count="6" id="aacIoHkraJ6R"}
``` python
class CIFAR_Image:
    def __init__(self, image, label):
        # Dimensions of image for reconstruction - not really necessary
        # for this dataset, but some datasets may include images of
        # varying sizes
        self.channels = image.shape[2]
        self.size = image.shape[:2]

        self.image = image.tobytes()
        self.label = label

    def get_image(self):
        """ Returns the image as a numpy array. """
        image = np.frombuffer(self.image, dtype=np.uint8)
        return image.reshape(*self.size, self.channels)
```
:::

::: {.cell .markdown id="urQ4kE2LaJ6R"}
Kelas `CIFAR_Image` merupakan fungsi yang merepresentasikan gambar dari dataset CIFAR. Metode `__init__` menginisialisasi objek dengan atribut-atribut seperti jumlah channel, dimensi gambar, dan isi gambar yang dikonversi menjadi bytes. Lalu, juga menyimpan label dari gambar tersebut. Metode `get_image` untuk mengembalikan gambar dalam bentuk array numpy sesuai dengan dimensi dan channel yang disimpan, jadi dapat dengan mudah mengelola dan memproses gambar-gambar dari dataset CIFAR dalam format yang sesuai untuk analisis lebih lanjut.
:::

::: {.cell .code execution_count="7" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="_3c98TyNdWqO" outputId="82c21759-0c00-4a39-eccb-a88b7531093a"}
``` python
pip install lmdb
```

::: {.output .stream .stdout}
    Collecting lmdb
      Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 299.2/299.2 kB 1.7 MB/s eta 0:00:00
    db
    Successfully installed lmdb-1.4.1
:::
:::

::: {.cell .code execution_count="8" id="NCs8_6KoaJ6R"}
``` python
import lmdb
import pickle

def store_single_lmdb(image, image_id, label):
    """ Stores a single image to a LMDB.
        Parameters:
        ---------------
        image       image array, (32, 32, 3) to be stored
        image_id    integer unique ID for image
        label       image label
    """
    map_size = image.nbytes * 10

    # Create a new LMDB environment
    env = lmdb.open(str(lmdb_dir / f"single_lmdb"), map_size=map_size)

    # Start a new write transaction
    with env.begin(write=True) as txn:
        # All key-value pairs need to be strings
        value = CIFAR_Image(image, label)
        key = f"{image_id:08}"
        txn.put(key.encode("ascii"), pickle.dumps(value))
    env.close()
```
:::

::: {.cell .markdown id="szoU7shHaJ6S"}
Kode merupakan fungsi untuk menyimpan gambar ke dalama basis data LMDB. Fungsi `store_single_lmdb` menerima tiga argumen: gambar sebagai array (32, 32, 3), ID unik gambar sebagai integer, dan label gambar. Bagian awal, variabel `map_size` dihitung berdasarkan ukuran gambar untuk menentukan ukuran memori yang dialokasikan untuk basis data LMDB. Lingkungan LMDB baru dibuat menggunakan `lmdb.open` dengan parameter `map_size` yang dihitung sebelumnya. Se;anjutnya, transaksi tulis baru dimulai menggunakan `env.begin(write=True)`. Semua pasangan kunci-nilai berupa string, sehingga gambar dan label diwakili sebagai objek `CIFAR_Image` dan di-serialize menggunakan modul `pickle`. Selanjutnya, fungsi memasukkan data ke dalam LMDB menggunakan `txn.put`. Setelah transaksi selesai, LMDB ditutup dengan `env.close()`.
:::

::: {.cell .code execution_count="9" id="FPWDMfkFaJ6S"}
``` python
import h5py

def store_single_hdf5(image, image_id, label):
    """ Stores a single image to an HDF5 file.
        Parameters:
        ---------------
        image       image array, (32, 32, 3) to be stored
        image_id    integer unique ID for image
        label       image label
    """
    # Create a new HDF5 file
    file = h5py.File(hdf5_dir / f"{image_id}.h5", "w")

    # Create a dataset in the file
    dataset = file.create_dataset(
        "image", np.shape(image), h5py.h5t.STD_U8BE, data=image
    )
    meta_set = file.create_dataset(
        "meta", np.shape(label), h5py.h5t.STD_U8BE, data=label
    )
    file.close()
```
:::

::: {.cell .markdown id="_8f_tmVJaJ6S"}
File HDF5 dibuat menggunakan `h5py.File` dengan nama file sesuai dengan ID gambar. Dataset gambar dan dataset metadata (label) dibuat di dalam file HDF5 menggunakan `create_dataset`. Dataset gambar diberi nama \"image\" dengan tipe data `h5py.h5t.STD_U8BE` yang merepresentasikan unsigned 8-bit integer big endian. Sedangkan dataset metadata diberi nama \"meta\" dengan tipe data yang sama seperti gambar. Data dari gambar dan metadata dimasukkan ke dalam dataset masing-masing. Setelah dataset dibuat dan diisi dengan data, file HDF5 ditutup dengan `file.close()`. 
:::

::: {.cell .code execution_count="10" id="tte2CdshaJ6T"}
``` python
_store_single_funcs = dict(
    disk=store_single_disk, lmdb=store_single_lmdb, hdf5=store_single_hdf5
)
```
:::

::: {.cell .markdown id="VMCGuXgHaJ6T"}
Kode membuat sebuah kamus `_store_single_funcs` yang berisi tiga fungsi: `store_single_disk`, `store_single_lmdb`, dan `store_single_hdf5`, yang masing-masing terkait dengan metode penyimpanan data yang berbeda (`disk`, `lmdb`, dan `hdf5`). Penggunaan ini untuk memilih metode penyimpanan yang sesuai dengan kebutuhan dengan memanggil fungsi yang sesuai dengan kunci yang diinginkan.
:::

::: {.cell .code execution_count="11" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="7zNsKUooaJ6T" outputId="12e04c27-ddbb-4b9d-fa12-52fbbb53b9d1"}
``` python
from timeit import timeit

store_single_timings = dict()

for method in ("disk", "lmdb", "hdf5"):
    t = timeit(
        "_store_single_funcs[method](image, 0, label)",
        setup="image=images[0]; label=labels[0]",
        number=1,
        globals=globals(),
    )
    store_single_timings[method] = t
    print(f"Method: {method}, Time usage: {t}")
```

::: {.output .stream .stdout}
    Method: disk, Time usage: 0.012546100999998089
    Method: lmdb, Time usage: 0.005812736000009977
    Method: hdf5, Time usage: 0.0017124150000000782
:::
:::

::: {.cell .markdown id="1CNI0RT6aJ6T"}
Dapat disimpulkan, metode LMDB merupakan yang paling cepat dalam hal waktu penyimpanan data, diikuti oleh metode hdf5, dan metode disk yang merupakan yang memakan waktu paling lama di antara ketiganya.
:::

::: {.cell .code execution_count="12" id="mMmbwiyQaJ6U"}
``` python
def store_many_disk(images, labels):
    """ Stores an array of images to disk
        Parameters:
        ---------------
        images       images array, (N, 32, 32, 3) to be stored
        labels       labels array, (N, 1) to be stored
    """
    num_images = len(images)

    # Save all the images one by one
    for i, image in enumerate(images):
        Image.fromarray(image).save(disk_dir / f"{i}.png")

    # Save all the labels to the csv file
    with open(disk_dir / f"{num_images}.csv", "w") as csvfile:
        writer = csv.writer(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        for label in labels:
            # This typically would be more than just one value per row
            writer.writerow([label])

def store_many_lmdb(images, labels):
    """ Stores an array of images to LMDB.
        Parameters:
        ---------------
        images       images array, (N, 32, 32, 3) to be stored
        labels       labels array, (N, 1) to be stored
    """
    num_images = len(images)

    map_size = num_images * images[0].nbytes * 10

    # Create a new LMDB DB for all the images
    env = lmdb.open(str(lmdb_dir / f"{num_images}_lmdb"), map_size=map_size)

    # Same as before — but let's write all the images in a single transaction
    with env.begin(write=True) as txn:
        for i in range(num_images):
            # All key-value pairs need to be Strings
            value = CIFAR_Image(images[i], labels[i])
            key = f"{i:08}"
            txn.put(key.encode("ascii"), pickle.dumps(value))
    env.close()

def store_many_hdf5(images, labels):
    """ Stores an array of images to HDF5.
        Parameters:
        ---------------
        images       images array, (N, 32, 32, 3) to be stored
        labels       labels array, (N, 1) to be stored
    """
    num_images = len(images)

    # Create a new HDF5 file
    file = h5py.File(hdf5_dir / f"{num_images}_many.h5", "w")

    # Create a dataset in the file
    dataset = file.create_dataset(
        "images", np.shape(images), h5py.h5t.STD_U8BE, data=images
    )
    meta_set = file.create_dataset(
        "meta", np.shape(labels), h5py.h5t.STD_U8BE, data=labels
    )
    file.close()
```
:::

::: {.cell .markdown id="nf7l_RHfaJ6i"}
Fungsi `store_many_disk` untuk menyimpan sebuah array gambar beserta label ke dalam disk dengan menyimpan setiap gambar dalam format .png dan labelnya dalam sebuah file .csv terpisah. Fungsi `store_many_lmdb` untuk menyimpan sebuah array gambar beserta labelnya ke dalam basis data LMDB dengan menyimpan setiap gambar dan labelnya dalam sebuah transaksi tunggal di dalam basis data tersebut. Fungsi `store_many_hdf5` untuk menyimpan sebuah array gambar beserta labelnya ke dalam sebuah file HDF5 dengan menyimpan seluruh array gambar dalam satu dataset bernama \"images\" dan seluruh array label dalam satu dataset bernama \"meta\". Ketiga fungsi tersebut menerima dua argumen: array gambar dengan dimens (N, 32, 32, 3) dan array label dengan dimensi (N, 1), di mana N adalah jumlah gambar yang akan disimpan.
:::

::: {.cell .code execution_count="13" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="yiiYGZUnaJ6i" outputId="30cac330-dd60-4c5e-8c63-483b7984e3ac"}
``` python
cutoffs = [10, 100, 1000, 10000, 100000]

# Let's double our images so that we have 100,000
images = np.concatenate((images, images), axis=0)
labels = np.concatenate((labels, labels), axis=0)

# Make sure you actually have 100,000 images and labels
print(np.shape(images))
print(np.shape(labels))
```

::: {.output .stream .stdout}
    (100000, 32, 32, 3)
    (100000,)
:::
:::

::: {.cell .markdown id="i4bQrWgTaJ6i"}
Hasil output adalah menggandakan jumlah gambar (images) dan label (labels) dari sebelumnya. Terdapat 100.000 gambar dalam bentuk array dengan dimensi (100000, 32, 32, 3) dan 100.000 label dalam bentuk array dengan dimensi (100000,), ini dibuktikan oleh hasil print dari `np.shape(images)` yang menunjukkan dimensi (100000, 32, 32, 3) dan `np.shape(labels)` yang menunjukkan dimensi (100000,).
:::

::: {.cell .code execution_count="14" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="Q4R8Jks4aJ6j" outputId="cf017b15-2af2-4568-8c89-6d3e4ba15eaa"}
``` python
_store_many_funcs = dict(
    disk=store_many_disk, lmdb=store_many_lmdb, hdf5=store_many_hdf5
)

from timeit import timeit

store_many_timings = {"disk": [], "lmdb": [], "hdf5": []}

for cutoff in cutoffs:
    for method in ("disk", "lmdb", "hdf5"):
        t = timeit(
            "_store_many_funcs[method](images_, labels_)",
            setup="images_=images[:cutoff]; labels_=labels[:cutoff]",
            number=1,
            globals=globals(),
        )
        store_many_timings[method].append(t)

        # Print out the method, cutoff, and elapsed time
        print(f"Method: {method}, Time usage: {t}")
```

::: {.output .stream .stdout}
    Method: disk, Time usage: 0.007395501999994281
    Method: lmdb, Time usage: 0.006236420999996994
    Method: hdf5, Time usage: 0.0019489409999948748
    Method: disk, Time usage: 0.03931480800000031
    Method: lmdb, Time usage: 0.009144467000012924
    Method: hdf5, Time usage: 0.016223940000003267
    Method: disk, Time usage: 0.4102962599999955
    Method: lmdb, Time usage: 0.03490387400000827
    Method: hdf5, Time usage: 0.004860675000003312
    Method: disk, Time usage: 5.4313113689999994
    Method: lmdb, Time usage: 0.567127806000002
    Method: hdf5, Time usage: 0.040642376000008085
    Method: disk, Time usage: 54.57821855499999
    Method: lmdb, Time usage: 3.954720846000015
    Method: hdf5, Time usage: 0.7876956559999826
:::
:::

::: {.cell .markdown id="BxOAgb7oaJ6j"}
Output menunjukkan waktu yang dibutuhkan (dalam detik) oleh setiap metode (`disk`, `lmdb`, `hdf5`) untuk menyimpan sejumlah gambar dan label yang berbeda berdasarkan nilai cutoff yang ditentukan sebelumnya (10, 100, 1000, 10000, 100000). Saat nilai cutoff adalah 10, waktu yang dibutuhkan `disk` sekitar 0.043 detik, untuk metode `lmdb` adalah sekitar 0.013 detik, dan metode `hdf5` adalah sekitar 0.054 detik. Ketika nilai cutoff adalah 100, waktu yang dibutuhkan untuk `disk` naik menjadi sekitar 0.167 detik, sementara `lmdb` menjadi sekitar 0.005 detik, dan metode `hdf5` menjadi sekitar 0.002 detik. Ketiga, ketika nilai cutoff terus bertambah hingga mencapai 100000, waktu yang dibutuhkan meningkat secara signifikan. Metode `disk` membutuhkan waktu sekitar 12.147 detik, metode `lmdb` membutuhkan waktu sekitar 0.297 detik, dan metode `hdf5` membutuhkan waktu sekitar 0.026 detik. Maka dapat disimpulkan, performa relatif dari masing-masing metode berbeda tergantung pada jumlah data yang disimpan. Metode `lmdb` memiliki waktu eksekusi yang lebih singkat dibandingkan dengan `disk` dan `hdf5`.
:::

::: {.cell .code execution_count="15" colab="{\"base_uri\":\"https://localhost:8080/\",\"height\":1000}" id="ZfrjrQ03aJ6j" outputId="2627c280-426b-4e6c-c6bf-12d7dc4274df"}
``` python
import matplotlib.pyplot as plt

def plot_with_legend(
    x_range, y_data, legend_labels, x_label, y_label, title, log=False
):
    """ Displays a single plot with multiple datasets and matching legends.
        Parameters:
        --------------
        x_range         list of lists containing x data
        y_data          list of lists containing y values
        legend_labels   list of string legend labels
        x_label         x axis label
        y_label         y axis label
    """
    plt.style.use("seaborn-whitegrid")
    plt.figure(figsize=(10, 7))

    if len(y_data) != len(legend_labels):
        raise TypeError(
            "Error: number of data sets does not match number of labels."
        )

    all_plots = []
    for data, label in zip(y_data, legend_labels):
        if log:
            temp, = plt.loglog(x_range, data, label=label)
        else:
            temp, = plt.plot(x_range, data, label=label)
        all_plots.append(temp)

    plt.title(title)
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.legend(handles=all_plots)
    plt.show()

# Getting the store timings data to display
disk_x = store_many_timings["disk"]
lmdb_x = store_many_timings["lmdb"]
hdf5_x = store_many_timings["hdf5"]

plot_with_legend(
    cutoffs,
    [disk_x, lmdb_x, hdf5_x],
    ["PNG files", "LMDB", "HDF5"],
    "Number of images",
    "Seconds to store",
    "Storage time",
    log=False,
)

plot_with_legend(
    cutoffs,
    [disk_x, lmdb_x, hdf5_x],
    ["PNG files", "LMDB", "HDF5"],
    "Number of images",
    "Seconds to store",
    "Log storage time",
    log=True,
)
```

::: {.output .stream .stderr}
    <ipython-input-15-99d89538a067>:15: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.
      plt.style.use("seaborn-whitegrid")
:::

::: {.output .display_data}
![](vertopal_60608bceba3a4c58a2d26d86c79f433d/0dc39917a1c6d1f1370f6c1f1a8ad2b6acd2a80e.png)
:::

::: {.output .stream .stderr}
    <ipython-input-15-99d89538a067>:15: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.
      plt.style.use("seaborn-whitegrid")
:::

::: {.output .display_data}
![](vertopal_60608bceba3a4c58a2d26d86c79f433d/e4ec5874fc246a6d3b5b4352ff1b80bac20f4c83.png)
:::
:::

::: {.cell .code execution_count="16" id="8wkrrHJ_aJ6j"}
``` python
def read_single_disk(image_id):
    """ Stores a single image to disk.
        Parameters:
        ---------------
        image_id    integer unique ID for image

        Returns:
        ----------
        image       image array, (32, 32, 3) to be stored
        label       associated meta data, int label
    """
    image = np.array(Image.open(disk_dir / f"{image_id}.png"))

    with open(disk_dir / f"{image_id}.csv", "r") as csvfile:
        reader = csv.reader(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        label = int(next(reader)[0])

    return image, label
```
:::

::: {.cell .markdown id="hlcKFkcgaJ6j"}
Fungsi `read_single_disk` untuk membaca gambar dan label yang disimpan dalam disk. Gambar dibaca menggunakan `Image.open` dan diubah menjadi array numpy. Labelnya dibaca dari file CSV yang sesuai dengan ID gambar. Selanjutnya, fungsi mengembalikan gambar dalam bentuk array dengan dimensi (32, 32, 3) dan label dalam bentuk integer. Dengan demikian, fungsi ini memungkinkan untuk membaca kembali data gambar dan label yang disimpan dalam format yang ditentukan sebelumnya dalam disk.
:::

::: {.cell .code execution_count="17" id="rmO1D0dxaJ6k"}
``` python
def read_single_lmdb(image_id):
    """ Stores a single image to LMDB.
        Parameters:
        ---------------
        image_id    integer unique ID for image

        Returns:
        ----------
        image       image array, (32, 32, 3) to be stored
        label       associated meta data, int label
    """
    # Open the LMDB environment
    env = lmdb.open(str(lmdb_dir / f"single_lmdb"), readonly=True)

    # Start a new read transaction
    with env.begin() as txn:
        # Encode the key the same way as we stored it
        data = txn.get(f"{image_id:08}".encode("ascii"))
        # Remember it's a CIFAR_Image object that is loaded
        cifar_image = pickle.loads(data)
        # Retrieve the relevant bits
        image = cifar_image.get_image()
        label = cifar_image.label
    env.close()

    return image, label
```
:::

::: {.cell .markdown id="792wJ8ZVaJ6k"}
Kode `read_single_lmdb` untuk membaca sebuah gambar dan label yang tersimpan dalam basis data LMDB untuk menerima satu argumen yaitu `image_id`, yang merupakan ID unik untuk gambar yang akan dibaca. Lingkungan LMDB dibuka dengan mode hanya baca (readonly=True) menggunakan `lmdb.open`. Selanjutnya, transaksi baca baru dimulai dengan `env.begin()`. Key yang digunakan diencode dengan metode yang sama saat menyimpan data. Data gambar diambil dari basis data LMDB dengan menggunakan `txn.get`. Data tersebut di-decode menggunakan `pickle.loads`. Lalu, gambar direkonstruksi menggunakan metode `get_image()` dari objek CIFAR_Image, dan label diambil langsung dari atribut label pada objek tersebut. Kemudian, lingkungan LMDB ditutup dan gambar beserta label dikembalikan sebagai output dari fungsi.
:::

::: {.cell .code execution_count="18" id="F5FgxcN2aJ6k"}
``` python
def read_single_hdf5(image_id):
    """ Stores a single image to HDF5.
        Parameters:
        ---------------
        image_id    integer unique ID for image

        Returns:
        ----------
        image       image array, (32, 32, 3) to be stored
        label       associated meta data, int label
    """
    # Open the HDF5 file
    file = h5py.File(hdf5_dir / f"{image_id}.h5", "r+")

    image = np.array(file["/image"]).astype("uint8")
    label = int(np.array(file["/meta"]).astype("uint8"))

    return image, label
```
:::

::: {.cell .markdown id="8aX_8S7ZaJ6k"}
Fungsi `read_single_hdf5` untuk membaca gambar dan label yang tersimpan dalam file HDF5 untuk menerima satu argumen yaitu `image_id`, yang merupakan ID unik untuk gambar yang dibaca. File HDF5 dibuka, lalu gambar dibaca dari dataset \"image\" dalam file HDF5 dan diubah menjadi array numpy dengan tipe data `uint8` (unsigned integer 8-bit). Kemudian, label dibaca dari dataset \"meta\" dan diubah menjadi integer dengan tipe data `uint8`. Setelah membaca, file HDF5 ditutup dan gambar beserta label dikembalikan sebagai output dari fungsi.
:::

::: {.cell .code execution_count="19" id="d5ZpAaeQaJ6k"}
``` python
_read_single_funcs = dict(
    disk=read_single_disk, lmdb=read_single_lmdb, hdf5=read_single_hdf5
)
```
:::

::: {.cell .markdown id="NqbGLWNvaJ6k"}
Kode `_read_single_funcs` membuat sebuah kamus yang berisi tiga fungsi: `read_single_disk`, `read_single_lmdb`, dan `read_single_hdf5`, masing-masing terkait dengan metode membaca data dari lokasi penyimpanan yang berbeda (`disk`, `lmdb`, dan `hdf5`).
:::

::: {.cell .code execution_count="20" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="CPZVT-zSaJ6k" outputId="95d13519-4d30-4063-eb3d-6c80b7092674"}
``` python
from timeit import timeit

read_single_timings = dict()

for method in ("disk", "lmdb", "hdf5"):
    t = timeit(
        "_read_single_funcs[method](0)",
        setup="image=images[0]; label=labels[0]",
        number=1,
        globals=globals(),
    )
    read_single_timings[method] = t
    print(f"Method: {method}, Time usage: {t}")
```

::: {.output .stream .stdout}
    Method: disk, Time usage: 0.0009021090000089771
    Method: lmdb, Time usage: 0.0002776039999901059
    Method: hdf5, Time usage: 0.0016816110000092976
:::
:::

::: {.cell .markdown id="4s_95vXTaJ6l"}
Output menunjukkan waktu yang dibutuhkan (dalam detik) oleh masing-masing metode (`disk`, `lmdb`, `hdf5`) untuk membaca sebuah gambar dan label dari lokasi penyimpanan yang berbeda. Hasil menyimpulkan bahwa metode `hdf5` merupakan yang paling cepat dalam waktu membaca data, diikuti oleh metode `lmdb`, dan metode `disk`.
:::

::: {.cell .code execution_count="21" id="ZddmD-DuaJ6l"}
``` python
def read_many_disk(num_images):
    """ Reads image from disk.
        Parameters:
        ---------------
        num_images   number of images to read

        Returns:
        ----------
        images      images array, (N, 32, 32, 3) to be stored
        labels      associated meta data, int label (N, 1)
    """
    images, labels = [], []

    # Loop over all IDs and read each image in one by one
    for image_id in range(num_images):
        images.append(np.array(Image.open(disk_dir / f"{image_id}.png")))

    with open(disk_dir / f"{num_images}.csv", "r") as csvfile:
        reader = csv.reader(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        for row in reader:
            labels.append(int(row[0]))
    return images, labels

def read_many_lmdb(num_images):
    """ Reads image from LMDB.
        Parameters:
        ---------------
        num_images   number of images to read

        Returns:
        ----------
        images      images array, (N, 32, 32, 3) to be stored
        labels      associated meta data, int label (N, 1)
    """
    images, labels = [], []
    env = lmdb.open(str(lmdb_dir / f"{num_images}_lmdb"), readonly=True)

    # Start a new read transaction
    with env.begin() as txn:
        # Read all images in one single transaction, with one lock
        # We could split this up into multiple transactions if needed
        for image_id in range(num_images):
            data = txn.get(f"{image_id:08}".encode("ascii"))
            # Remember that it's a CIFAR_Image object
            # that is stored as the value
            cifar_image = pickle.loads(data)
            # Retrieve the relevant bits
            images.append(cifar_image.get_image())
            labels.append(cifar_image.label)
    env.close()
    return images, labels

def read_many_hdf5(num_images):
    """ Reads image from HDF5.
        Parameters:
        ---------------
        num_images   number of images to read

        Returns:
        ----------
        images      images array, (N, 32, 32, 3) to be stored
        labels      associated meta data, int label (N, 1)
    """
    images, labels = [], []

    # Open the HDF5 file
    file = h5py.File(hdf5_dir / f"{num_images}_many.h5", "r+")

    images = np.array(file["/images"]).astype("uint8")
    labels = np.array(file["/meta"]).astype("uint8")

    return images, labels

_read_many_funcs = dict(
    disk=read_many_disk, lmdb=read_many_lmdb, hdf5=read_many_hdf5
)
```
:::

::: {.cell .markdown id="vK39H-yCaJ6l"}
Kode mendefinisikan tiga fungsi, `read_many_disk`, `read_many_lmdb`, dan `read_many_hdf5`, untuk membaca sejumlah gambar dan label dari berbagai lokasi penyimpanan (disk, LMDB, HDF5). Fungsi `read_many_disk` membaca gambar dan label dari disk. Fungsi `read_many_lmdb` membaca gambar dan label dari basis data LMDB dengan membuka transaksi baca menggunakan modul lmdb. Fungsi `read_many_hdf5` membaca gambar dan label dari file HDF5 dengan membuka file HDF5 dan membaca dataset \"images\" dan \"meta\" dari file tersebut. Kamus `_read_many_funcs` untuk mengelompokkan ketiga fungsi pembacaan berdasarkan lokasi penyimpanan yang berbeda (disk, LMDB, HDF5).
:::

::: {.cell .code execution_count="22" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="v7VkeeQ1aJ6s" outputId="34afa2a9-8002-48b2-d7fb-f20a29a34df4"}
``` python
from timeit import timeit

read_many_timings = {"disk": [], "lmdb": [], "hdf5": []}

for cutoff in cutoffs:
    for method in ("disk", "lmdb", "hdf5"):
        t = timeit(
            "_read_many_funcs[method](num_images)",
            setup="num_images=cutoff",
            number=1,
            globals=globals(),
        )
        read_many_timings[method].append(t)

        # Print out the method, cutoff, and elapsed time
        print(f"Method: {method}, No. images: {cutoff}, Time usage: {t}")
```

::: {.output .stream .stdout}
    Method: disk, No. images: 10, Time usage: 0.006645504000005076
    Method: lmdb, No. images: 10, Time usage: 0.00048318099999278274
    Method: hdf5, No. images: 10, Time usage: 0.001567037000000937
    Method: disk, No. images: 100, Time usage: 0.05944305200000599
    Method: lmdb, No. images: 100, Time usage: 0.002380230999989408
    Method: hdf5, No. images: 100, Time usage: 0.001980457000001934
    Method: disk, No. images: 1000, Time usage: 0.5046740360000115
    Method: lmdb, No. images: 1000, Time usage: 0.03461824600000796
    Method: hdf5, No. images: 1000, Time usage: 0.004023425000013958
    Method: disk, No. images: 10000, Time usage: 4.102075581000008
    Method: lmdb, No. images: 10000, Time usage: 0.12356936899999482
    Method: hdf5, No. images: 10000, Time usage: 0.028392960999980232
    Method: disk, No. images: 100000, Time usage: 41.404140050000024
    Method: lmdb, No. images: 100000, Time usage: 1.2840579010000113
    Method: hdf5, No. images: 100000, Time usage: 0.5161999530000116
:::
:::

::: {.cell .markdown id="oSwxx-BbaJ6s"}
Kode mengukur waktu yang dibutuhkan untuk membaca sejumlah gambar dari berbagai metode penyimpanan (disk, LMDB, HDF5) dengan menggunakan `timeit`. Hasil pengukuran waktu disimpan dalam kamus `read_many_timings`. Setiap iterasi, kode melakukan pengukuran waktu untuk membaca sejumlah gambar (`num_images`) berdasarkan metode penyimpanan dalam variabel `method`. Pengukuran waktu dilakukan sekali untuk setiap metode. Kode juga mencetak informasi tentang metode penyimpanan, jumlah gambar yang dibaca, dan waktu yang dibutuhkan untuk pembacaan tersebut.
:::
